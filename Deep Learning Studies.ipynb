{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Error Functions\n",
    "\n",
    "### Discrete vs Continuos\n",
    "\n",
    "In order to use gradient descent, it needs to have a continuos error function. To do this, we need to move from discrete predictions to continuos.\n",
    "\n",
    "In order to channge from discrete to continuos predictions we need to change the activation function. From the discrete step function:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x \\geq 0\\\\\n",
    "    0 & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To the Sigmoid Function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\dfrac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "The softmax function is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.\n",
    "\n",
    "Linear function scores: $Z_1, \\ldots, Z_n$\n",
    "\n",
    "$$P(\\textrm{class i}) = \\dfrac{e^{z_i}}{e^{z_1} + \\ldots + e^{z_n}}$$\n",
    "\n",
    "For $n = 2$, the Softmax function will be the same as the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likehood\n",
    "\n",
    "#### Cross-Entropy\n",
    "\n",
    "It's the negative of the logatithm of the products of probabilities. A higher cross-entropy implies a lower probability for an event.\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{m} y_i\\ln{(p_i)} + (1 - y_i)\\ln{(1 - p_i)}$$\n",
    "\n",
    "$$\n",
    "\\textrm{CE}[(1, 1, 0), (0.8, 0.7, 0.1)] = 0.69 \\\\\n",
    "\\textrm{CE}[(0, 0, 1), (0.8, 0.7, 0.1)] = 5.12\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0\n",
    "    \n",
    "    for i in range(0, len(Y)):\n",
    "        result -= Y[i] * np.log(P[i]) + (1 - Y[i]) * np.log(1 - P[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simplified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class Cross-Entropy\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{n}\\sum_{j = 1}^{m} y_{ij}\\ln{(p_{ij})}$$\n",
    "\n",
    "$m$ being the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Error Function\n",
    "\n",
    "$$\\textrm{Error Function} = - \\dfrac{1}{m} \\sum_{i=1}^{m} (1 - y_i)\\ln{(1 - \\hat{y_i})} + y_i\\ln{(\\hat{y_i})}$$\n",
    "\n",
    "Since $\\hat{y_i}$ is given by the sigmoid of the linear function $Wx + b$, then the total formula is:\n",
    "\n",
    "$$E(W,b) = - \\dfrac{1}{m} \\sum_{i=1}^{m} (1 - y_i)\\ln{(1 - \\sigma(Wx^{(i)} + b))} + y_i\\ln{(\\sigma(Wx^{(i)} + b))}$$\n",
    "\n",
    "Then to minimize the error we use Gradient descent.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Uses derivatives to minimize the error function.\n",
    "\n",
    "The derivative of the sigmoid function:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "And the derivati of the error $E$ at a point $x$, with respect to the weight $w_j$:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial b}E = -(y - \\hat{y})$$\n",
    "\n",
    "A small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:\n",
    "\n",
    "$$w_i' \\gets w_i - \\alpha[-(y - \\hat{y})x_i]$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$w_i' \\gets w_i + \\alpha(y - \\hat{y})x_i$$\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$$b' \\gets b + \\alpha(y - \\hat{y})$$\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "1. Start with random weights: $w_1, \\ldots, w_n, b$\n",
    "2. For every point ($x_1, \\ldots, x_n$):\n",
    "    1. For $i = 1 \\ldots n$:\n",
    "        1. Update $w' \\gets w_1 - \\alpha(\\hat{y} - y)x_i $\n",
    "        2. Update $b' \\gets b - \\alpha(\\hat{y} - y)$\n",
    "3. Repeat until error is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "Feedforward is the process neural networks use to turn the input into an output.\n",
    "\n",
    "$$\\hat{y} = \\sigma \\circ W^{(2)} \\circ \\sigma \\circ W^{(1)}(x)$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "1. Doing a feedforward operation.\n",
    "2. Comparing the output of the model with the desired output.\n",
    "3. Calculating the error.\n",
    "4. Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "5. Use this to update the weights, and get a better model.\n",
    "6. Continue this until we have a model that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Squared Errors\n",
    "\n",
    "$$E = \\dfrac{1}{2}\\sum_{\\mu}\\sum_j[y_j^{\\mu} - \\hat{y}_j^{\\mu}]^2$$\n",
    "\n",
    "First, the inside sum over $j$. This variable $j$ represents the output units of the network. So this inside sum is saying for each output unit, find the difference between the true value $y$ and the predicted value from the network $\\hat{y}$, then square the difference, then sum up all those squares.\n",
    "\n",
    "Then the other sum over $\\mu$ is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
    "\n",
    "The SSE (Sum of Squared Errors) is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "Since the weights will just go wherever the gradient takes them, they can end up where the error is low, but not the lowest. These spots are called local minima. If the weights are initialized with the wrong values, gradient descent could lead the weights into a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent implementation\n",
    "\n",
    "One weight update can be calculated as:\n",
    "\n",
    "$$\\Delta w_i = \\eta \\delta x_i$$\n",
    "\n",
    "with the error term $\\delta$ as:\n",
    "\n",
    "$$\\delta = (y - \\hat{y})f'(h) = (y - \\hat{y})f'(\\sum w_ix_i)$$\n",
    "\n",
    "In the above equation $(yâˆ’\\hat{y})$ is the output error, and $f'(h)$ refers to the derivative of the activation function, $f(h)$. We'll call that derivative the output gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "\n",
    "# Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "# x[0]*w[0] + x[1]*w[1] + ... + x[n]*w[n]\n",
    "\n",
    "# Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# Calculate the error term\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('data/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.loc[data.index[sample]], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2634711464098939\n",
      "Train loss:  0.22351730609465992\n",
      "Train loss:  0.20940833581363916\n",
      "Train loss:  0.20359343681746\n",
      "Train loss:  0.20087567293629532\n",
      "Train loss:  0.19945113185302385\n",
      "Train loss:  0.1986335569379286\n",
      "Train loss:  0.19813048800765415\n",
      "Train loss:  0.19780393857042008\n",
      "Train loss:  0.19758300602502077\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "def sigmoid_prime(x):\n",
    "    output = sigmoid(x)\n",
    "    \n",
    "    return output * (1 - output)\n",
    "    \n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        h = np.dot(x, weights)\n",
    "        \n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(h)\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * sigmoid_prime(h)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += learnrate * error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += (learnrate * del_w) / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
