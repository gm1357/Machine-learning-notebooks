{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Error Functions\n",
    "\n",
    "### Discrete vs Continuos\n",
    "\n",
    "In order to use gradient descent, it needs to have a continuos error function. To do this, we need to move from discrete predictions to continuos.\n",
    "\n",
    "In order to channge from discrete to continuos predictions we need to change the activation function. From the discrete step function:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x \\geq 0\\\\\n",
    "    0 & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To the Sigmoid Function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\dfrac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "The softmax function is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.\n",
    "\n",
    "Linear function scores: $Z_1, \\ldots, Z_n$\n",
    "\n",
    "$$P(\\textrm{class i}) = \\dfrac{e^{z_i}}{e^{z_1} + \\ldots + e^{z_n}}$$\n",
    "\n",
    "For $n = 2$, the Softmax function will be the same as the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likehood\n",
    "\n",
    "#### Cross-Entropy\n",
    "\n",
    "It's the negative of the logatithm of the products of probabilities. A higher cross-entropy implies a lower probability for an event.\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{m} y_i\\ln{(p_i)} + (1 - y_i)\\ln{(1 - p_i)}$$\n",
    "\n",
    "$$\n",
    "\\textrm{CE}[(1, 1, 0), (0.8, 0.7, 0.1)] = 0.69 \\\\\n",
    "\\textrm{CE}[(0, 0, 1), (0.8, 0.7, 0.1)] = 5.12\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0\n",
    "    \n",
    "    for i in range(0, len(Y)):\n",
    "        result -= Y[i] * np.log(P[i]) + (1 - Y[i]) * np.log(1 - P[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simplified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class Cross-Entropy\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{n}\\sum_{j = 1}^{m} y_{ij}\\ln{(p_{ij})}$$\n",
    "\n",
    "$m$ being the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Error Function\n",
    "\n",
    "$$\\textrm{Error Function} = - \\dfrac{1}{m} \\sum_{i=1}^{m} (1 - y_i)\\ln{(1 - \\hat{y_i})} + y_i\\ln{(\\hat{y_i})}$$\n",
    "\n",
    "Since $\\hat{y_i}$ is given by the sigmoid of the linear function $Wx + b$, then the total formula is:\n",
    "\n",
    "$$E(W,b) = - \\dfrac{1}{m} \\sum_{i=1}^{m} (1 - y_i)\\ln{(1 - \\sigma(Wx^{(i)} + b))} + y_i\\ln{(\\sigma(Wx^{(i)} + b))}$$\n",
    "\n",
    "Then to minimize the error we use Gradient descent.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Uses derivatives to minimize the error function.\n",
    "\n",
    "The derivative of the sigmoid function:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "And the derivati of the error $E$ at a point $x$, with respect to the weight $w_j$:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial b}E = -(y - \\hat{y})$$\n",
    "\n",
    "A small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:\n",
    "\n",
    "$$w_i' \\gets w_i - \\alpha[-(y - \\hat{y})x_i]$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$w_i' \\gets w_i + \\alpha(y - \\hat{y})x_i$$\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$$b' \\gets b + \\alpha(y - \\hat{y})$$\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "1. Start with random weights: $w_1, \\ldots, w_n, b$\n",
    "2. For every point ($x_1, \\ldots, x_n$):\n",
    "    1. For $i = 1 \\ldots n$:\n",
    "        1. Update $w' \\gets w_1 - \\alpha(\\hat{y} - y)x_i $\n",
    "        2. Update $b' \\gets b - \\alpha(\\hat{y} - y)$\n",
    "3. Repeat until error is small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
