{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Error Functions\n",
    "\n",
    "### Discrete vs Continuos\n",
    "\n",
    "In order to use gradient descent, it needs to have a continuos error function. To do this, we need to move from discrete predictions to continuos.\n",
    "\n",
    "In order to channge from discrete to continuos predictions we need to change the activation function. From the discrete step function:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x \\geq 0\\\\\n",
    "    0 & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To the Sigmoid Function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\dfrac{1}{1 + \\mathrm{e}^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "The softmax function is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.\n",
    "\n",
    "Linear function scores: $Z_1, \\ldots, Z_n$\n",
    "\n",
    "$$P(\\textrm{class i}) = \\dfrac{e^{z_i}}{e^{z_1} + \\ldots + e^{z_n}}$$\n",
    "\n",
    "For $n = 2$, the Softmax function will be the same as the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likehood\n",
    "\n",
    "#### Cross-Entropy\n",
    "\n",
    "It's the negative of the logatithm of the products of probabilities. A higher cross-entropy implies a lower probability for an event.\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{m} y_i\\ln{(p_i)} + (1 - y_i)\\ln{(1 - p_i)}$$\n",
    "\n",
    "$$\n",
    "\\textrm{CE}[(1, 1, 0), (0.8, 0.7, 0.1)] = 0.69 \\\\\n",
    "\\textrm{CE}[(0, 0, 1), (0.8, 0.7, 0.1)] = 5.12\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0\n",
    "    \n",
    "    for i in range(0, len(Y)):\n",
    "        result -= Y[i] * np.log(P[i]) + (1 - Y[i]) * np.log(1 - P[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simplified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851790109107685"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1, 1, 0], (0.8, 0.7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class Cross-Entropy\n",
    "\n",
    "$$\\textrm{Cross-Entropy} = - \\sum_{i = 1}^{n}\\sum_{j = 1}^{m} y_{ij}\\ln{(p_{ij})}$$\n",
    "\n",
    "$m$ being the number of classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
