{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Two ways of doing linear regression:\n",
    " \n",
    " - By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    " - By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "\n",
    "More specifically, the squared (or absolute) trick, when applied to a point, gives us some values to add to the weights of the model. We can add these values, update our weights, and then apply the squared (or absolute) trick on the next point. Or we can calculate these values for all the points, add them, and then update the weights with the sum of these values.\n",
    "\n",
    "The latter is called **batch gradient descent**. The former is called **stochastic gradient descent**.\n",
    "\n",
    "If your data is huge, both are a bit slow, computationally. The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called **mini-batch gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### absolute trick\n",
    "\n",
    "$$y = (w_1 + p\\alpha)x + (w_2 + p\\alpha)$$\n",
    "\n",
    "Sendo $p$ a distancia do ponto ao eixo y e $\\alpha$ o coeficiente de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square trick\n",
    "\n",
    "$$y = (w_1 + p(q - q')\\alpha)x +(w_2 + (q - q')\\alpha)$$\n",
    "\n",
    "Sendo $p$ a distancia do ponto ao eixo y e $\\alpha$ o coeficiente de aprendizado.\\\n",
    "$q$ o valor y do ponto e $q'$ o valor y da equação no valor x do ponto fornecido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of Error Function (Gradient Descent)\n",
    "\n",
    "$$w_i \\rightarrow w_i - \\alpha\\dfrac{\\partial}{\\partial w_i}Error$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "$$Error = \\dfrac{\\sum^m_{i=1}|{y - y'}|}{m}$$\n",
    "\n",
    "Sendo $m$ o número de pontos, $y$ a cordenada y do ponto e $y'$ a cordenada y da linha no x do ponto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    " \n",
    "$$Error = \\dfrac{\\sum^m_{i=1}{(y - y')^2}}{2m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Mini-batch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEStep(X, y, W, b, learn_rate = 0.005):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute errors\n",
    "    y_pred = np.matmul(X, W) + b\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # compute steps\n",
    "    W_new = W + learn_rate * np.matmul(error, X)\n",
    "    b_new = b + learn_rate * error.sum()\n",
    "    \n",
    "    return W_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):\n",
    "    \"\"\"\n",
    "    This function performs mini-batch gradient descent on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    batch_size : how many data points will be sampled for each iteration\n",
    "    learn_rate : learning rate\n",
    "    num_iter : number of batches used\n",
    "\n",
    "    Returns\n",
    "    regression_coef : array of slopes and intercepts generated by gradient\n",
    "      descent procedure\n",
    "    \"\"\"\n",
    "    n_points = X.shape[0]\n",
    "    W = np.zeros(X.shape[1]) # coefficients\n",
    "    b = 0 # intercept\n",
    "    \n",
    "    # run iterations\n",
    "    regression_coef = [np.hstack((W,b))]\n",
    "    for _ in range(num_iter):\n",
    "        batch = np.random.choice(range(n_points), batch_size)\n",
    "        X_batch = X[batch,:]\n",
    "        y_batch = y[batch]\n",
    "        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n",
    "        regression_coef.append(np.hstack((W,b)))\n",
    "    \n",
    "    return regression_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.loadtxt('mini-batch-data.csv', delimiter = ',')\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "regression_coef = miniBatchGD(X, y)\n",
    "    \n",
    "# plot the results\n",
    "plt.figure()\n",
    "X_min = X.min()\n",
    "X_max = X.max()\n",
    "counter = len(regression_coef)\n",
    "for W, b in regression_coef:\n",
    "    counter -= 1\n",
    "    color = [1 - 0.92 ** counter for _ in range(3)]\n",
    "    plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)\n",
    "plt.scatter(X, y, zorder = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = m_1x_1 + m_2x_2 + m_3x_3 + \\ldots + m_nx_n + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with Linear Regression using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "bmi_life_data = pd.read_csv('data/bmi_and_life_expectancy.csv')\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Make a prediction using the model\n",
    "laos_life_exp = bmi_life_model.predict([[21.07931]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the predicted life expectancy with BMI 21.07931:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60.31564716]]\n"
     ]
    }
   ],
   "source": [
    "print(laos_life_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "\n",
    "\n",
    "prediction = model.predict(sample_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.68284712]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression warnings\n",
    "\n",
    " - Linear Regression works best when the data is linear, if the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features, or use another kind of model.\n",
    " - Linear Regression is Sensitive to Outliers, if your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = w_1x^{n-1} + w_2x^{n-2} + \\ldots + w_{n-1}x + w_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the **mean absolute** or **squared error**, then take the derivative in respect of the n variables and use **gradient descent** to modify these n weights in order to minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the data to predictor and outcome variables\n",
    "train_data = pd.read_csv('data/polynomial_regression.csv')\n",
    "X = train_data['Var_X'].values.reshape((20, 1))\n",
    "y = train_data['Var_Y']\n",
    "\n",
    "# Create polynomial features\n",
    "poly_feat = PolynomialFeatures(4)\n",
    "X_poly = poly_feat.fit_transform(X, y)\n",
    "\n",
    "# Make and fit the polynomial regression model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Used to penalize more complex models and choose the simpler one, even if the error is a little bigger.\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Given the function:\n",
    "\n",
    "$$y = m_1x_1 + m_2x_2 + m_3x_3 + \\ldots + m_nx_n + b$$\n",
    "\n",
    "We add the sum absolute value of the coeficients multiplied by $\\lambda$ (to adjust how much it's desired to punish the model for it's complexity) to the error:\n",
    "\n",
    "$$Error = Error + (|m_1| + |m_2| + |m_3| + \\ldots + |m_n|)\\lambda$$\n",
    "\n",
    "## L2 Regularization\n",
    "\n",
    "It follows the same idea of L1 method, but instead of adding the absolute value, it adds the squared value:\n",
    "\n",
    "$$Error = Error + (m_1^2 + m_2^2 + m_3^2 + \\ldots + m_n^2)\\lambda$$\n",
    "\n",
    "## L1 vs L2\n",
    "\n",
    "| L1 Regularization | L2 Regularization |\n",
    "|---|---|\n",
    "| Computationally Inefficient<br>(unless data is sparse) | Computationally Efficient |\n",
    "| Sparse Outputs | Non-Sparse Outrputs |\n",
    "| Feature Selection | No feature Selection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          2.35793224  2.00441646 -0.05511954 -3.92808318  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "train_data = pd.read_csv('data/regularization-data.csv', header = None)\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "reg_coef = lasso_reg.coef_\n",
    "print(reg_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is a way of transforming your data into a common range of values. There are two common scalings:\n",
    "\n",
    " - Standardizing\n",
    " - Normalizing\n",
    "\n",
    "### Standardizing\n",
    "\n",
    "Standardizing is completed by taking each value of your column, subtracting the mean of the column, and then dividing by the standard deviation of the column. This type of feature scaling is by far the most common of all techniques.\n",
    "\n",
    "```\n",
    "df[\"height_standard\"] = (df[\"height\"] - df[\"height\"].mean()) / df[\"height\"].std()\n",
    "```\n",
    "\n",
    "This will create a new \"standardized\" column where each value is a comparison to the mean of the column, and a new, standardized value can be interpreted as the number of standard deviations the original height was from the mean.\n",
    "\n",
    "### Normalizing\n",
    "\n",
    "With normalizing, data are scaled between 0 and 1.\n",
    "\n",
    "```\n",
    "df[\"height_normal\"] = (df[\"height\"] - df[\"height\"].min()) /     \\\n",
    "                      (df[\"height\"].max() - df['height'].min())\n",
    "```\n",
    "\n",
    "### When to use Feature Scaling\n",
    "\n",
    "In many machine learning algorithms, the result will change depending on the units of your data. This is especially true in two specific cases:\n",
    "\n",
    " - When your algorithm uses a distance-based metric to predict.\n",
    " - When you incorporate regularization.\n",
    " \n",
    "Feature scaling can speed up convergence of your machine learning algorithms, which is an important consideration when you scale machine learning applications.\n",
    "\n",
    "#### DIstance based metrics\n",
    "\n",
    "One common supervised learning technique that is based on the distance points are from one another called **Support Vector Machines (SVMs)**. Another technique that involves distance based methods to determine a prediction is **k-nearest neighbors (k-nn)**. With either of these techniques, choosing not to scale your data may lead to drastically different (and likely misleading) ending predictions. \n",
    "\n",
    "#### Regularization\n",
    "\n",
    "When you start introducing regularization, you will again want to scale the features of your model. The penalty on particular coefficients in regularized linear regression techniques depends largely on the scale associated with the features. When one feature is on a small range, say from 0 to 10, and another is on a large range, say from 0 to 1 000 000, applying regularization is going to unfairly punish the feature with the small range. Features with small ranges need to have larger coefficients compared to features with large ranges in order to have the same effect on the outcome of the data. (Think about how $ab = ba$ for two numbers $a$ and $b$.) Therefore, if regularization could remove one of those two features with the same net increase in error, it would rather remove the small-ranged feature with the large coefficient, since that would reduce the regularization term the most.\n",
    "\n",
    "Again, this means you will want to scale features any time you are applying regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           3.90753617   9.02575748  -0.         -11.78303187\n",
      "   0.45340137]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data = pd.read_csv('data/regularization-data.csv', header = None)\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "lasso_reg.fit(X_scaled, y)\n",
    "\n",
    "reg_coef = lasso_reg.coef_\n",
    "print(reg_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Linear boundaries\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + b = 0$$\n",
    "$$Wx + b = 0$$\n",
    "$$W = (w_1, w_2)$$\n",
    "$$x = (x_1, x_2)$$\n",
    "$$y = label: 0\\ or\\ 1$$\n",
    "\n",
    "$\\hat{y}$ (the prediction) will be 1 if $Wx + b  \\geqslant  0$ and 0 if $Wx + b < 0$\n",
    "\n",
    "### Higher dimensions\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b = 0$$\n",
    "$$Wx + b = 0$$\n",
    "$$W = (w_1, w_2, \\ldots, w_n)$$\n",
    "$$x = (x_1, x_2, \\ldots, x_n)$$\n",
    "$$y = label: 0\\ or\\ 1$$\n",
    "\n",
    "$\\hat{y}$ (the prediction) will be 1 if $Wx + b  \\geqslant  0$ and 0 if $Wx + b < 0$\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "Perceptrons are nodes that represents the formulas above and can be used as logical operators. \n",
    "\n",
    "### AND Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                  -1.5                    0          Yes\n",
      "       0          1                  -0.5                    0          Yes\n",
      "       1          0                  -0.5                    0          Yes\n",
      "       1          1                   0.5                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1\n",
    "weight2 = 1\n",
    "bias = -1.5\n",
    "\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                   0.0                    1          Yes\n",
      "       0          1                  -1.0                    0          Yes\n",
      "       1          0                   0.0                    1          Yes\n",
      "       1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1\n",
    "bias = 0.0\n",
    "\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Perceptron\n",
    "\n",
    "The XOR operator needs to be done using a multi-layer perceptron consisting of operators AND, NOT (or NAND) and OR. This multi-layer perceptron can be considered a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron algorithm\n",
    "\n",
    " 1. Start with random weights: $w_1, w_2, \\ldots, w_n, b$\n",
    " 2. For every misclassified point ($x_1, \\ldots, x_n$):\n",
    "     1. If *predction* = 0:\n",
    "         1. For $i = 1\\ldots n$:\n",
    "             1. Change $w_i$ to $w_i + \\alpha x_i$\n",
    "         2. Change $b$ to $b + \\alpha$\n",
    "     2. If *prediction* = 1:\n",
    "         1. For $i = 1\\ldots n$:\n",
    "             1. Change $w_i$ to $w_i - \\alpha x_i$\n",
    "         2. Change $b$ to $b - \\alpha$\n",
    "         \n",
    "$\\alpha$ being the learning rate coeficient.\n",
    "\n",
    "#### Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        \n",
    "        if y[i] - y_hat == 1:\n",
    "            W[0] += learn_rate * X[i][0]\n",
    "            W[1] += learn_rate * X[i][1]\n",
    "            b += learn_rate\n",
    "        elif y[i] - y_hat == -1:\n",
    "            W[0] -= learn_rate * X[i][0]\n",
    "            W[1] -= learn_rate * X[i][1]\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Entropy\n",
    "\n",
    "$$Entropy = - \\dfrac{m}{m + n}\\log_2(\\dfrac{m}{m + n}) - \\dfrac{n}{m + n}\\log_2(\\dfrac{n}{m + n})$$\n",
    "\n",
    "We can state this in terms of probabilities instead using:\n",
    "\n",
    "$$p_1 = \\dfrac{m}{m + n}$$\n",
    "\n",
    "$$p_2 = \\dfrac{n}{m + n}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$Entropy = - p_1\\log_2(p_1) - p_2\\log_2(p_2)$$\n",
    "\n",
    "This entropy equation can be extended to the multi-class case, where we have three or more possible values:\n",
    "\n",
    "$$Entropy = - p_1\\log_2(p_1) - p_2\\log_2(p_2) - \\ldots - p_n\\log_2(p_n) = \\sum_{i=1}^{n}p_i\\log_2(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculateEntropy(arr):\n",
    "    entropy = 0\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        p = calculateProbability(i, arr)\n",
    "        entropy -= p * np.log2(p)\n",
    "        \n",
    "    return entropy\n",
    "        \n",
    "def calculateProbability(index, arr):\n",
    "    sum = 0\n",
    "    for i in range(len(arr)):\n",
    "        sum += arr[i]\n",
    "        \n",
    "    return arr[index] / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3346791410515946\n"
     ]
    }
   ],
   "source": [
    "print(calculateEntropy([8, 3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain Formula\n",
    "\n",
    "In general, the average entropy for the child groups will need to be a weighted average, based on the number of cases in each child group. That is, for $m$ items in the first child group and $n$ items in the second child group, the information gain is:\n",
    "\n",
    "$$Information Gain = Entropy(parent) - [\\dfrac{m}{m + n}Entropy(child_1) + \\dfrac{n}{n + m}Entropy(child_2)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInformationGain(entropy_parent, arr_entropy_child, arr_child_weights):\n",
    "    entropy_children = 0\n",
    "    sum_weights = sum(arr_child_weights)\n",
    "    \n",
    "    for i in range(len(arr_entropy_child)):\n",
    "            entropy_children += (arr_entropy_child[i] * arr_child_weights[i]) / sum_weights\n",
    "        \n",
    "    return entropy_parent - entropy_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEntropyChild(arr_data, filter_type, filter_value, compare_number = False):\n",
    "    \n",
    "    if compare_number:\n",
    "        data = arr_data[arr_data[filter_type] < filter_value]\n",
    "        data_not = arr_data[arr_data[filter_type] >= filter_value]\n",
    "    else:\n",
    "        data = arr_data[arr_data[filter_type] == filter_value]\n",
    "        data_not = arr_data[arr_data[filter_type] != filter_value]\n",
    "\n",
    "    entropy_child = calculateEntropy([\n",
    "        np.count_nonzero(data['Species'] == 'Mobug'), \n",
    "        np.count_nonzero(data['Species'] == 'Lobug')\n",
    "    ])\n",
    "\n",
    "    entropy_child_not = calculateEntropy([\n",
    "        np.count_nonzero(data_not['Species'] == 'Mobug'), \n",
    "        np.count_nonzero(data_not['Species'] == 'Lobug')\n",
    "    ])\n",
    "\n",
    "    return entropy_child, entropy_child_not, len(data), len(data_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain with brown criteria: 0.06157292259666325\n",
      "Information gain with blue criteria: 0.000589596275060833\n",
      "Information gain with green criteria: 0.042776048498108565\n",
      "Information gain with 17 criteria: 0.11260735516748976\n",
      "Information gain with 20 criteria: 0.10073322588651734\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "bugs_data = pd.read_csv('data/ml-bugs.csv')\n",
    "\n",
    "p_mobug = np.count_nonzero(bugs_data['Species'] == 'Mobug') / len(bugs_data['Species'])\n",
    "p_lobug = np.count_nonzero(bugs_data['Species'] == 'Lobug') / len(bugs_data['Species'])\n",
    "\n",
    "entropy_parent = calculateEntropy([\n",
    "    np.count_nonzero(bugs_data['Species'] == 'Mobug'), \n",
    "    np.count_nonzero(bugs_data['Species'] == 'Lobug')\n",
    "])\n",
    "\n",
    "entropy_child_brown, entropy_child_not_brown, \\\n",
    "    len_brown, len_not_brown = calculateEntropyChild(bugs_data, 'Color', 'Brown')\n",
    "\n",
    "print('Information gain with brown criteria: ' \n",
    "      + str(calculateInformationGain(\n",
    "          entropy_parent, \n",
    "          [entropy_child_brown, entropy_child_not_brown], \n",
    "          [len_brown, len_not_brown]\n",
    "      ))\n",
    "     )\n",
    "\n",
    "entropy_child_blue, entropy_child_not_blue, \\\n",
    "    len_blue, len_not_blue = calculateEntropyChild(bugs_data, 'Color', 'Blue')\n",
    "\n",
    "print('Information gain with blue criteria: ' \n",
    "      + str(calculateInformationGain(\n",
    "          entropy_parent, \n",
    "          [entropy_child_blue, entropy_child_not_blue], \n",
    "          [len_blue, len_not_blue]\n",
    "      ))\n",
    "     )\n",
    "\n",
    "entropy_child_green, entropy_child_not_green, \\\n",
    "    len_green, len_not_green = calculateEntropyChild(bugs_data, 'Color', 'Green')\n",
    "\n",
    "print('Information gain with green criteria: ' \n",
    "      + str(calculateInformationGain(\n",
    "          entropy_parent, \n",
    "          [entropy_child_green, entropy_child_not_green], \n",
    "          [len_green, len_not_green]\n",
    "      ))\n",
    "     )\n",
    "\n",
    "entropy_child_17, entropy_child_not_17, \\\n",
    "    len_17, len_not_17 = calculateEntropyChild(bugs_data, 'Length (mm)', 17, True)\n",
    "\n",
    "print('Information gain with 17 criteria: ' \n",
    "      + str(calculateInformationGain(\n",
    "          entropy_parent, \n",
    "          [entropy_child_17, entropy_child_not_17], \n",
    "          [len_17, len_not_17]\n",
    "      ))\n",
    "     )\n",
    "\n",
    "entropy_child_20, entropy_child_not_20, \\\n",
    "    len_20, len_not_20 = calculateEntropyChild(bugs_data, 'Length (mm)', 20, True)\n",
    "\n",
    "print('Information gain with 20 criteria: ' \n",
    "      + str(calculateInformationGain(\n",
    "          entropy_parent, \n",
    "          [entropy_child_20, entropy_child_not_20], \n",
    "          [len_20, len_not_20]\n",
    "      ))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Decision Trees\n",
    "\n",
    "In order to create decision trees that will generalize to new problems well, we can tune a number of different aspects about the trees. We call the different aspects of a decision tree \"hyperparameters\". These are some of the most important hyperparameters used in decision trees:\n",
    "\n",
    "### Maximum Depth\n",
    "\n",
    "The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. A tree of maximum length $k$ can have at most $2^k$ leaves.\n",
    "\n",
    "### Minimum number of samples to split\n",
    "\n",
    "A node must have at least `min_samples_split` samples in order to be large enough to split. If a node has fewer samples than `min_samples_split` samples, it will not be split, and the splitting process stops. However, `min_samples_split` doesn't control the minimum size of leaves.\n",
    "\n",
    "### Minimum number of samples per leaf\n",
    "\n",
    "When splitting a node, one could run into the problem of having 99 samples in one of them, and 1 on the other. This will not take us too far in our process, and would be a waste of resources and time. If we want to avoid this, we can set a minimum for the number of samples we allow on each leaf.\n",
    "\n",
    "This number can be specified as an integer or as a float. If it's an integer, it's the minimum number of samples allowed in a leaf. If it's a float, it's the minimum percentage of samples allowed in a leaf. For example, 0.1, or 10%, implies that a particular split will not be allowed if one of the leaves that results contains less than 10% of the samples in the dataset.\n",
    "\n",
    "If a threshold on a feature results in a leaf that has fewer samples than `min_samples_leaf`, the algorithm will not *allow* that split, but it may perform a split on the same feature at a *different threshold*, that *does* satisfy min_samples_leaf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/decision-tree-predict.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# You won't need to, but if you'd like, play with hyperparameters such\n",
    "# as max_depth and min_samples_leaf and see what they do to the decision\n",
    "# boundary.\n",
    "model = DecisionTreeClassifier(max_depth = 7)\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "acc = accuracy_score(y, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "| Know      | Inferred |\n",
    "|:---------:|:----------:|\n",
    "| $P(A)$      | $P(A|R)$  |\n",
    "| $P(R|A)$ |          |\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$P(A|R) = \\dfrac{P(A)P(R|A)}{P(A)P(R|A) + P(B)P(R|B)}$$\n",
    "\n",
    "$P(A)$ is the prior probrability (what we knew before we knew $R$ occorred) and $P(A|R)$ is the posterior (what we inferred after we new what occurred)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes algorithm\n",
    "\n",
    "The algorithm uses the two techniques described bellow (naive assumption and conditional probability) to find a probability in a more naive an computationally easier manner.\n",
    "\n",
    "### Naive assumption\n",
    "\n",
    "It's assumed that:\n",
    "\n",
    "$$P(A\\cap B) = P(A)P(B)$$\n",
    "\n",
    "Even if P(A) and P(B) are dependant, that's why it's naive.\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "$$P(A|B)P(B) = P(B|A)P(A)$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$P(A|B) \\propto P(B|A)P(A)$$\n",
    "\n",
    "($\\propto$ is the symbol to \"is proportional to\").\n",
    "\n",
    "To get the actual probability (that sums to 1), it's needed to divide the resulting proportional probabilities by the sum of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Q: Suppose you have a bag with **three standard 6-sided dice** with face values [1,2,3,4,5,6] and **two non-standard 6-sided dice** with face values [2,3,3,4,4,5]. Someone draws a die from the bag, rolls it, and announces it was a 3. What is the probability that the die that was rolled was a standard die?\n",
    "\n",
    "R:\n",
    "We are trying to get:\n",
    "\n",
    "$$P(s|3) = ?$$\n",
    "\n",
    "So by the conditional probability technique:\n",
    "\n",
    "$$P(s|3) \\propto P(3|s)P(s)$$\n",
    "\n",
    "And we can calculate $P(3|s)$ and $P(s)$ as follow:\n",
    "\n",
    "$$P(3|s) = \\dfrac{3}{18} = \\dfrac{1}{6}$$\n",
    "\n",
    "$$P(s) = \\dfrac{3}{5}$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$P(s|3) \\propto \\dfrac{1}{6}\\dfrac{3}{5} = \\dfrac{3}{30} = \\dfrac{1}{10}$$\n",
    "\n",
    "And to normalize, first we have to calculate the probability of getting a non-standard die when we get a three:\n",
    "\n",
    "$$P(ns|3) \\propto P(3|ns)P(ns) = \\dfrac{1}{3}\\dfrac{2}{5} = \\dfrac{2}{15}$$\n",
    "\n",
    "And then we divide the resulting probabilities by their sum:\n",
    "\n",
    "$$P(s|3) = \\dfrac{\\dfrac{1}{10}}{\\dfrac{1}{10} + \\dfrac{2}{15}} = \\dfrac{30}{70} = \\dfrac{3}{7}$$\n",
    "\n",
    "$$P(ns|3) = \\dfrac{\\dfrac{2}{15}}{\\dfrac{1}{10} + \\dfrac{2}{15}} = \\dfrac{60}{105} = \\dfrac{4}{7}$$\n",
    "\n",
    "And we get that $P(s|3) = \\dfrac{3}{7}$, we can verify that it's a valid probability by summing with $P(ns|3) = \\dfrac{4}{7}$ and noticing it results to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "SVMs are a popular algorithm used for classification problems. There are three different ways SVMs can be implemented:\n",
    "\n",
    "\n",
    " - Maximum Margin Classifier\n",
    " - Classification with Inseparable Classes\n",
    " - Kernel Methods\n",
    "\n",
    "## Maximum Margin Classifier\n",
    "\n",
    "When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors).\n",
    "\n",
    "### Classification Error\n",
    "\n",
    "<img src=\"images/class_error.png\" width=\"800\" height=\"700\">\n",
    "\n",
    "### Margin Error\n",
    "\n",
    "<img src=\"images/mar_error.png\" width=\"800\" height=\"700\">\n",
    "\n",
    "### Error Function\n",
    "\n",
    "$$Error = Classification Error + Margin Error$$\n",
    "\n",
    "And we minimize it using gradient descent.\n",
    "\n",
    "## Classification with Inseparable Classes\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable. For this reason, we introduced a new hyper-parameter called **C**.\n",
    "\n",
    "### The C parameter\n",
    "\n",
    "The **C** hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary. The value of C ranges between 0 and infinity.\n",
    "\n",
    "**C** is a constant that's multiplied by the classification error on the Error function to change the focus that is desired.\n",
    "\n",
    "$$Error = C (Classification Error) + Margin Error$$\n",
    "\n",
    " - Large C: Classifies points well. May have small margin\n",
    " - Small C: Large margin. May make classification errors.\n",
    " \n",
    "**Note: when C is too large for a particular set of data, you might not get convergence at all because your data cannot be separated with the small number of errors allotted with such a large value of C.**\n",
    "\n",
    "<img src=\"images/c_parameter.png\" width=\"800\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Methods\n",
    "\n",
    "Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear.\n",
    " \n",
    "### Kernel Trick\n",
    "\n",
    "The kernel trick is used when just a line won't do the job. It envolves using a higher dimension to solve the problem. \n",
    "\n",
    "Say $x$ and $y$ is the **linear kernel**, then the **polynomial kernel** (degree 2) will be: $x$, $x^2$, $y$, $y^2$ and $xy$.\n",
    "\n",
    "<img src=\"images/kernel_trick.png\" width=\"800\" height=\"700\">\n",
    "<img src=\"images/kernel_trick_2.png\" width=\"800\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF (radial basis function) Kernel\n",
    "\n",
    "By far the most popular kernel. The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another hyper-parameter **gamma**.\n",
    "\n",
    "#### $\\gamma$ parameter\n",
    "\n",
    "A large $\\gamma$ will give a narrow curve, a small $\\gamma$ will give a wide curve.\n",
    "\n",
    "$$\\gamma = \\dfrac{1}{2\\sigma^2}$$\n",
    "\n",
    "$\\sigma$ being the width of the curve.\n",
    "\n",
    "<img src=\"images/rbf.png\" width=\"800\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines in sklearn\n",
    "\n",
    "#### SVC Hyperparameters\n",
    "\n",
    "When we define the model, we can specify the hyperparameters. The most common ones are:\n",
    "\n",
    " - C: The C parameter.\n",
    " - kernel: The kernel. The most common ones are 'linear', 'poly', and 'rbf'.\n",
    " - degree: If the kernel is polynomial, this is the maximum degree of the monomials in the kernel.\n",
    " - gamma : If the kernel is rbf, this is the gamma parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/svc.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "model = SVC(kernel='rbf', gamma=27)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "acc = accuracy_score(y, y_pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Ensemble methods ensembles the models seen above in a way that makes the combination of these models better at predicting than the individual models. Commonly the \"weak\" learners you use are decision trees. In fact the default for most ensemble methods is a decision tree in sklearn. There are two main ensemble methods:\n",
    "\n",
    " - Bagging\n",
    " - Boosting\n",
    " \n",
    "## Bias–variance tradeoff\n",
    "\n",
    "**Bias**: When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad.\n",
    "\n",
    "<img src=\"images/bias.svg\" width=\"800\" height=\"700\">\n",
    "\n",
    "**Variance**: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "\n",
    "<img src=\"images/variance.png\" width=\"800\" height=\"700\">\n",
    "\n",
    "By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "This method is used to not overfit decision trees. It consists in picking random colomns of the dataset and making decision trees on them, then it uses this generated trees to predict on the given data, the most predict value resulted is the choosed for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Picks random points in the dataset and using a weak learner try to classify them. This is made multiple times. Then it adds the classifiers to obtain the final classifier.\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Using a weak learner try to classify the points in the dataset. Then punish the mistakenly classified points by making them bigger (Using weights). repeat the process more times, each time trying to better classify the bigger points. Then it adds the classifiers to obtain the final classifier.\n",
    "\n",
    "$$weight = \\ln{(\\dfrac{accuracy}{1 - accuracy})}$$\n",
    "\n",
    "Or, simplifying:\n",
    "\n",
    "$$weight = \\ln{(\\dfrac{\\#corrects}{\\#incorrects})}$$"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/d84ba39abc1b95f706e7956e0ca7bec0"
  },
  "gist": {
   "data": {
    "description": "Machine Learning Nanodegree studies.ipynb",
    "public": false
   },
   "id": "d84ba39abc1b95f706e7956e0ca7bec0"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
